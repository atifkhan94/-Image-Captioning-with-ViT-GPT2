{
    "cells": [{
            "cell_type": "markdown",
            "metadata": {},
            "source": ["# Image Captioning with ViT-GPT2\n",
                "\n",
                "This notebook demonstrates an image captioning system using the Vision Transformer (ViT) and GPT-2 models. The model generates natural language descriptions for input images by combining the power of vision transformers for image understanding and GPT-2 for text generation.\n",
                "\n",
                "## Features\n",
                "- Uses ViT (Vision Transformer) as the image encoder\n",
                "- Employs GPT-2 as the language model for caption generation\n",
                "- Integrates with Hugging Face transformers library\n",
                "- Provides example usage with sample images"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## Setup\n",
                "First, let's import the required libraries and set up our environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ["import torch\n",
                "import pandas as pd\n",
                "from PIL import Image\n",
                "import os\n",
                "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "torch.manual_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## Load Model and Processors\n",
                "We'll use the pre-trained ViT-GPT2 model for image captioning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ["def load_model_and_processors():\n",
                "    model_name = 'nlpconnect/vit-gpt2-image-captioning'\n",
                "    model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
                "    feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    return model, feature_extractor, tokenizer\n",
                "\n",
                "model, feature_extractor, tokenizer = load_model_and_processors()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## Caption Generation Function\n",
                "Let's define a function to generate captions for input images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ["def generate_caption(image_path, model, feature_extractor, tokenizer):\n",
                "    image = Image.open(image_path).convert('RGB')\n",
                "    pixel_values = feature_extractor(image, return_tensors='pt').pixel_values\n",
                "\n",
                "    generated_ids = model.generate(\n",
                "        pixel_values,\n",
                "        max_length=30,\n",
                "        num_beams=4,\n",
                "        early_stopping=True\n",
                "    )\n",
                "\n",
                "    generated_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
                "    return generated_caption"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## Example Usage\n",
                "Let's try the model on some sample images from our dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ["# Set paths\n",
                "image_dir = 'Images'\n",
                "\n",
                "# Get a few sample images\n",
                "sample_images = os.listdir(image_dir)[:5]\n",
                "\n",
                "# Generate and display captions\n",
                "for image_name in sample_images:\n",
                "    image_path = os.path.join(image_dir, image_name)\n",
                "    \n",
                "    # Display the image\n",
                "    image = Image.open(image_path)\n",
                "    display(image)\n",
                "    \n",
                "    # Generate and print the caption\n",
                "    caption = generate_caption(image_path, model, feature_extractor, tokenizer)\n",
                "    print(f'Generated caption: {caption}\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## Compare with Ground Truth\n",
                "Let's compare the generated captions with the original captions from our dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ["# Load original captions\n",
                "captions_df = pd.read_csv('captions.txt')\n",
                "\n",
                "# Create a dictionary of image to captions\n",
                "caption_dict = {}\n",
                "for _, row in captions_df.iterrows():\n",
                "    if row['image'] not in caption_dict:\n",
                "        caption_dict[row['image']] = []\n",
                "    caption_dict[row['image']].append(row['caption'])\n",
                "\n",
                "# Compare generated captions with original ones\n",
                "for image_name in sample_images:\n",
                "    print(f'Image: {image_name}')\n",
                "    \n",
                "    # Display original captions\n",
                "    print('Original captions:')\n",
                "    for caption in caption_dict[image_name]:\n",
                "        print(f'- {caption}')\n",
                "    \n",
                "    # Generate and display new caption\n",
                "    image_path = os.path.join(image_dir, image_name)\n",
                "    generated = generate_caption(image_path, model, feature_extractor, tokenizer)\n",
                "    print(f'\nGenerated caption:\n- {generated}\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## Conclusion\n",
                "This notebook demonstrated how to use the ViT-GPT2 model for image captioning. The model combines the power of Vision Transformers for image understanding with GPT-2's language generation capabilities to create natural descriptions of images.\n",
                "\n",
                "Key points:\n",
                "- The model effectively processes images and generates relevant captions\n",
                "- Captions are generated using beam search for better quality\n",
                "- The implementation is easy to use and can be integrated into various applications\n",
                "\n",
                "Feel free to experiment with different images and parameters to see how the model performs!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}